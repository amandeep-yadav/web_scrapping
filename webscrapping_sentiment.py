# -*- coding: utf-8 -*-
"""webscrapping_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-apFtncIifgJAFIlj16bxATVJ7WoyBff
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup as bs
import string
import textblob

import nltk
nltk.download('punkt')
import requests
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# df=pd.read_excel("Input.xlsx")

# df.shape

# n,m=df.shape

auditor=open('StopWords_Auditor.txt').read().upper().split()
currency=open('StopWords_Currencies.txt',errors='ignore').read().upper().split()
number=open('StopWords_DatesandNumbers.txt',errors='ignore').read().upper().split()
generic=open('StopWords_Generic.txt',errors='ignore').read().upper().split()
genericlong=open('StopWords_GenericLong.txt',errors='ignore').read().upper().split()
geographic=open('StopWords_Geographic.txt',errors='ignore').read().upper().split()
name=open('StopWords_Names.txt',errors='ignore').read().upper().split()

final_stop=[]
final_stop.extend(auditor)
final_stop.extend(currency)
final_stop.extend(number)
final_stop.extend(generic)
final_stop.extend(genericlong)
final_stop.extend(geographic)
final_stop.extend(name)

final_stop=final_stop

negative_word=open("negative-words.txt",encoding="utf-8",errors="ignore").read().upper().split()
positive_word=open("positive-words.txt",encoding="utf-8",errors="ignore").read().upper().split()



p_pronoun=["I","We","we","My","my","Ours","ours","Us","us"]
vowel=["A","E","I","O","U"]

pos_score=[]
neg_score=[]
polar=[]
subjective_score=[]
avg_sent_length=[]
percent_comp=[]
fog=[]
avgno_word=[]
complex_word=[]
word_count=[]
syllab=[]
personal=[]
avg_word_len=[]
urls=[] 
# you can put many as many you wants
for url in urls:
  x=requests.get(url)
  gt=bs(x.content,"lxml")
  text=gt.getText(strip=True)

  sentence=sent_tokenize(text)
  sent_length=len(sentence)

  # text=text.upper()
  clean_text=text.translate(str.maketrans("","",string.punctuation))
  token_word=word_tokenize(clean_text)

  total_word=len(token_word)
  pp=0
  for word in token_word:
    if word in p_pronoun:
      pp=pp+1


  count=0
  final_token_word=[]
  text=text.upper()
  clean_text=text.translate(str.maketrans("","",string.punctuation))
  token_word=word_tokenize(clean_text)

  for word in token_word:
    count=count+len(word)
    if word not in final_stop:
      final_token_word.append(word)
  pos=0
  neg=0
  cmp=0
  vw=0
  for word in final_token_word:
    if word[0] in vowel:
      if len(word)>1 and word[1]!='S' or word[1]!='D':
        vw=vw+1

    if word in positive_word:
      pos=pos+1
    if word in negative_word:
      neg=neg+1
    if word in negative_word and positive_word:
      cmp=cmp+1


  pos_score.append(pos)

  neg_score.append(neg)

  x=(pos-neg)/(pos+neg+.000001)
  polar.append(x)
  # save only polarity of the web_text if you want sentiment then change using if else statement and many more
  x=(pos+neg)/(total_word+.000001)
  subjective_score.append(x)

  y=total_word/(sent_length+.000001)
  avg_sent_length.append(y)
  avgno_word.append(y)

  complex_word.append(cmp)

  word_count.append(len(final_token_word))

  x=cmp/(total_word+.000001)*100
  percent_comp.append(x)

  z=.4*(x+y)
  fog.append(z)

  x=count/(len(final_token_word)+.00001)
  avg_word_len.append(x)

  personal.append(pp)
  syllab.append(vw)

# df=pd.DataFrame

data={
# "URL_ID":df["URL_ID"],
"URL":urls,
"POSITIVE SCORE":pos_score,
"NEGATIVE SCORE":neg_score,
"POLARITY SCORE":polar,
"SUBJECTIVITY SCORE":subjective_score,
"AVG SENTENCE LENGTH":avg_sent_length,
"PERCENTAGE OF COMPLEX WORDS":percent_comp,
"FOG INDEX":fog,
"AVG NUMBER OF WORDS PER SENTENCE":avgno_word,
"COMPLEX WORD COUNT":complex_word,
"WORD COUNT":word_count,
"SYLLABLE PER WORD":syllab,
"PERSONAL PRONOUNS":personal,
"AVG WORD LENGTH":(avg_word_len)
}
df1=pd.DataFrame(data)

df3=pd.read_excel("Output Data Structure.xlsx")
df3.head()

df1.to_csv('output.csv')

df1

df2=df1

df1.isna().sum()

df2["AVG SENTENCE LENGTH"]=df2["AVG SENTENCE LENGTH"].apply(np.int64)

df2["AVG NUMBER OF WORDS PER SENTENCE"]=df2["AVG NUMBER OF WORDS PER SENTENCE"]+1
df2["AVG NUMBER OF WORDS PER SENTENCE"]=df2["AVG NUMBER OF WORDS PER SENTENCE"].apply(np.int64)
df2["AVG WORD LENGTH"]=df2["AVG WORD LENGTH"]+1
df2["AVG WORD LENGTH"]=df2["AVG WORD LENGTH"].apply(np.int64)

df2

df2.to_csv('output_int.csv')

